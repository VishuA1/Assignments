{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Business Goal :**  To develop an AI-powered chatbot (Vajra.AI) for customer interaction and improve service delivery at Social Prachar Institute.\n",
        "\n",
        "**Purpose :**\n",
        "*   Assist students with queries related to courses, fees, institute details, internships, and more.\n",
        "*   Provide 24/7 customer support without the need for human intervention.\n",
        "*   Reduced manual workload for staff handling repetitive inquiries.\n",
        "*   Increased customer satisfaction through quick and accurate responses.:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fQdUTJI8NMyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing necessary libraries**"
      ],
      "metadata": {
        "id": "aKb-s71VOkRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install tensorflow\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz_YFTULXHZ1",
        "outputId": "263122bb-bdf7-44c9-d5f6-58acad1983ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSjpoLsWeHIs",
        "outputId": "383773b2-0669-4bd8-b02a-b0ee38b08591"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining User Intent and Response Patterns for Chatbot**"
      ],
      "metadata": {
        "id": "pdoNS51-Oy_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9grll_c2P1cz"
      },
      "outputs": [],
      "source": [
        "# Initialize Lemmatizer\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "# Example training data (same as provided)\n",
        "intents = [\n",
        "    {\n",
        "        \"intent\": \"greetings\",\n",
        "        \"patterns\": [\"Hello\", \"Hi\", \"Good day\", \"Hey\", \"How are you?\", \"Good morning\", \"Hi there\"],\n",
        "        \"responses\": [\n",
        "            \"Hello! Welcome to Social Prachar Institute. How can I assist you today?\",\n",
        "            \"Hi there! How can I help you with your queries about our courses?\",\n",
        "            \"Good day! I’m Vajra.AI, your guide to the Social Prachar Institute. What would you like to know?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"farewells\",\n",
        "        \"patterns\": [\"Goodbye\", \"Bye\", \"Thank you\", \"See you later\", \"Take care\", \"Farewell\"],\n",
        "        \"responses\": [\n",
        "            \"Thank you for visiting! Have a great day!\",\n",
        "            \"It was my pleasure to assist you. Take care and good luck with your learning journey!\",\n",
        "            \"Goodbye, and feel free to come back if you have more questions!\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"institute_details\",\n",
        "        \"patterns\": [\"What is the name of the institute?\", \"Where are you located?\", \"Tell me about your institute?\", \"What is Social Prachar Institute?\"],\n",
        "        \"responses\": [\n",
        "            \"The name of the institute is Social Prachar.\",\n",
        "            \"Social Prachar is an educational institute offering courses in Data Science & AI, Data Analytics, Python Full Stack, Java Full Stack, and AWS Developer. We focus on providing high-quality training to our students with a strong emphasis on hands-on experience and practical knowledge.\",\n",
        "            \"We are located at [hi-tech city road, 216, Manjeera Majestic Commercial, JNTU Rd, near JAWAHARLAL NEHRU TECHNOLOGICAL UNIVERSITY, Hyderabad, Telangana 500085].\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"course_info\",\n",
        "        \"patterns\": [\"What courses do you offer?\", \"Tell me about the courses\", \"What courses are available?\", \"Can you list your courses?\"],\n",
        "        \"responses\": [\n",
        "            \"We offer Data Science, Data Analytics, Python Full Stack, Java Full Stack, and AWS Developer courses.\",\n",
        "            \"Our courses include Data Science & AI, Data Analytics, Python Full Stack, Java Full Stack, and AWS Developer. You can learn more about each course on our website.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"fee_structure\",\n",
        "        \"patterns\": [\"How much does the course cost?\", \"What are the fees for the courses?\", \"Tell me the course fees.\"],\n",
        "        \"responses\": [\n",
        "            \"The fee for Data Science is 50k, and other courses are 30k.\",\n",
        "            \"Our courses are priced as follows: Data Science - 50k, Data Analytics - 30k, Python Full Stack - 30k, Java Full Stack - 30k, AWS Developer - 30k.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"internship_opportunity\",\n",
        "        \"patterns\": [\"Do you offer internships?\", \"Are there internship opportunities?\", \"Can I get an internship after completing the course?\"],\n",
        "        \"responses\": [\n",
        "            \"Yes, internships are available for some courses like Data Science and Python Full Stack.\",\n",
        "            \"We offer internships for selected courses such as Data Science and Python Full Stack. Internships are based on performance and course requirements.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"placement_assistance\",\n",
        "        \"patterns\": [\"Do you provide placement assistance?\", \"Will you help me get a job?\", \"Can you help me with jobs?\"],\n",
        "        \"responses\": [\n",
        "            \"Yes, we provide placement assistance, including resume building and interview preparation.\",\n",
        "            \"We offer placement assistance services such as resume building, mock interviews, job placement, and referrals.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"refund_policy\",\n",
        "        \"patterns\": [\"What is your refund policy?\", \"Can I get a refund if I don't like the course?\", \"Do you have a refund policy for your courses?\"],\n",
        "        \"responses\": [\n",
        "            \"We offer a 7-day refund policy if you're not satisfied with the course.\",\n",
        "            \"If you're not satisfied with the course, we offer a 7-day refund policy.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"demo_classes\",\n",
        "        \"patterns\": [\"Do you offer demo classes?\", \"Can I try a demo class?\", \"Where can I book a demo class?\"],\n",
        "        \"responses\": [\n",
        "            \"You can book your demo class by visiting [Social Prachar Institute](https://socialprachar.com/). Demo classes are available for all courses.\",\n",
        "            \"Demo classes are available for all courses. You can book your demo class on our website.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"payment_methods\",\n",
        "        \"patterns\": [\"What are the payment options?\", \"What payment methods do you accept?\", \"Can I pay online for the courses?\"],\n",
        "        \"responses\": [\n",
        "            \"We accept all types of payments.\",\n",
        "            \"You can pay via credit card, debit card, online banking, or other accepted methods. All payment options are listed on our website.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"error_handling\",\n",
        "        \"patterns\": [\"I didn't understand.\", \"I don't know what you mean.\", \"Can you explain it again?\"],\n",
        "        \"responses\": [\n",
        "            \"Sorry, I didn’t quite get that. Could you please rephrase your question?\",\n",
        "            \"I didn't quite understand. Could you please rephrase it?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"spelling_mistake\",\n",
        "        \"patterns\": [\"I think I made a spelling mistake.\", \"I might have typed it wrong.\", \"Could you check for spelling mistakes?\"],\n",
        "        \"responses\": [\n",
        "            \"I noticed a spelling mistake in your question. Could you please check and rephrase it?\",\n",
        "            \"There seems to be a spelling mistake. Could you please check and correct it?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"intent\": \"institute_name\",\n",
        "        \"patterns\": [\"What is the name of the institute?\", \"Which institute is this?\", \"Tell me the institute name\", \"What is the name of your institute?\"],\n",
        "        \"responses\": [\n",
        "            \"The name of the institute is Social Prachar.\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "9_HtnV2jO76q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: Tokenization and Lemmatization\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence_tokens = nltk.word_tokenize(sentence.lower())  # Tokenize\n",
        "    sentence_tokens = [lemmatizer.lemmatize(word) for word in sentence_tokens if word not in ignore_words]\n",
        "    return \" \".join(sentence_tokens)\n",
        "\n",
        "# Prepare data\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "class_names = []\n",
        "words = []\n",
        "ignore_words = ['?', '!', '.', ',']\n",
        "# Data preprocessing\n",
        "for intent in intents:\n",
        "    for pattern in intent['patterns']:\n",
        "        processed_sentence = preprocess_sentence(pattern)  # Apply preprocessing\n",
        "        training_sentences.append(processed_sentence)  # Use preprocessed sentence\n",
        "        training_labels.append(intent['intent'])\n",
        "\n",
        "        # Tokenizing and collecting all words for later vectorization\n",
        "        word_list = nltk.word_tokenize(pattern)  # Tokenize the sentence\n",
        "        words.extend(word_list)\n",
        "\n",
        "    if intent['intent'] not in class_names:\n",
        "        class_names.append(intent['intent'])\n",
        "\n",
        "# Lemmatize and remove duplicates from all the words\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))  # Remove duplicates and sort words\n",
        "\n",
        "# Encode the labels (intent labels to numeric values)\n",
        "# label_encoder = LabelEncoder()\n",
        "# labels = label_encoder.fit_transform(training_labels)\n",
        "\n",
        "# Using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(training_sentences).toarray()\n",
        "\n",
        "# # Convert labels to categorical format\n",
        "# y = training_labels\n",
        "\n",
        "X = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
        "y = pd.DataFrame(training_labels, columns=['intent'])"
      ],
      "metadata": {
        "id": "y8RsPY-3QSx4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training and Evaluation with Gaussian Naive Bayes**"
      ],
      "metadata": {
        "id": "xpysfEPZPIv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(y_pred)\n",
        "\n",
        "# Evaluate the model\n",
        "# Evaluate the model by comparing the predicted and true labels directly (no need for np.argmax)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k-6z8zMYhzS",
        "outputId": "1284e000-36d5-4ac2-e91c-fc73d3f1d5a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['institute_name' 'institute_name' 'institute_details' 'spelling_mistake'\n",
            " 'internship_opportunity']\n",
            "Accuracy: 0.400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting Intent and Providing Responses**"
      ],
      "metadata": {
        "id": "1s70V74PPWIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_intent(user_input):\n",
        "    # Tokenize and lemmatize the user input\n",
        "    user_input_tokens = nltk.word_tokenize(user_input.lower())\n",
        "    user_input_tokens = [lemmatizer.lemmatize(word) for word in user_input_tokens if word not in ignore_words]\n",
        "\n",
        "    # Convert the user input into the same numerical vector using the vectorizer\n",
        "    input_vector = vectorizer.transform([\" \".join(user_input_tokens)]).toarray()\n",
        "\n",
        "    # Predict the intent using the trained Naive Bayes model\n",
        "    prediction = nb_model.predict(input_vector)[0]\n",
        "\n",
        "    print(\"Intent:\",prediction)\n",
        "\n",
        "    # Rule-based responses for specific intents\n",
        "    for i in intents:\n",
        "        if i['intent'] == prediction:\n",
        "            return random.choice(i['responses'])\n",
        "\n",
        "    # Default response\n",
        "    return \"Sorry, I didn’t understand that. Could you please rephrase?\"\n",
        "\n",
        "# Main loop for interaction\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Bye! See you again.\")\n",
        "        break\n",
        "    response = predict_intent(user_input)\n",
        "    print(f\"Vajra.AI: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYXFKhcRYmp_",
        "outputId": "60cbd87a-4e05-4782-ca0c-2b3f8ff88122"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello\n",
            "Intent: greetings\n",
            "Vajra.AI: Hi there! How can I help you with your queries about our courses?\n",
            "You: what is the institute name\n",
            "Intent: institute_name\n",
            "Vajra.AI: The name of the institute is Social Prachar.\n",
            "You: where is the institute located\n",
            "Intent: institute_details\n",
            "Vajra.AI: The name of the institute is Social Prachar.\n",
            "You: where is the institute locate\n",
            "Intent: institute_details\n",
            "Vajra.AI: We are located at [hi-tech city road, 216, Manjeera Majestic Commercial, JNTU Rd, near JAWAHARLAL NEHRU TECHNOLOGICAL UNIVERSITY, Hyderabad, Telangana 500085].\n",
            "You: what are the courses you are offering\n",
            "Intent: course_info\n",
            "Vajra.AI: We offer Data Science, Data Analytics, Python Full Stack, Java Full Stack, and AWS Developer courses.\n",
            "You: data science fee\n",
            "Intent: fee_structure\n",
            "Vajra.AI: The fee for Data Science is 50k, and other courses are 30k.\n",
            "You: demo classes\n",
            "Intent: demo_classes\n",
            "Vajra.AI: Demo classes are available for all courses. You can book your demo class on our website.\n",
            "You: internship opportunities\n",
            "Intent: internship_opportunity\n",
            "Vajra.AI: We offer internships for selected courses such as Data Science and Python Full Stack. Internships are based on performance and course requirements.\n",
            "You: payment methods\n",
            "Intent: payment_methods\n",
            "Vajra.AI: We accept all types of payments.\n",
            "You: refund\n",
            "Intent: refund_policy\n",
            "Vajra.AI: We offer a 7-day refund policy if you're not satisfied with the course.\n",
            "You: thank you\n",
            "Intent: farewells\n",
            "Vajra.AI: Thank you for visiting! Have a great day!\n",
            "You: quit\n",
            "Bye! See you again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**saving the model**"
      ],
      "metadata": {
        "id": "921tPJ6nPcP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Example: Assuming 'model' is a trained machine learning model\n",
        "model = nb_model  # Your trained model here\n",
        "\n",
        "# Save the model to a .pkl file\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "jkYkr4nuZmfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**saving the vextorizer file**"
      ],
      "metadata": {
        "id": "zUldzXIFPfid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the vectorizer to a .pkl file\n",
        "with open('vectorizer.pkl', 'wb') as file:\n",
        "    pickle.dump(vectorizer, file)\n"
      ],
      "metadata": {
        "id": "FJ4Ig2YhYTjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NwukK5bFk3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}